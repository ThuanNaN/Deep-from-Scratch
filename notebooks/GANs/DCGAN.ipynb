{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "img_size = 32\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((img_size, img_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5],\n",
    "                                     std=[0.5])\n",
    "])\n",
    "\n",
    "images = torchvision.datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataloader = torch.utils.data.DataLoader(images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "img_shape = (channels, img_size, img_size)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_size = img_size // 4\n",
    "        self.fc = nn.Linear(latent_dim, 128*self.init_size**2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(x)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Descriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 16, kernel_size=3, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "        )\n",
    "\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(128 * ds_size ** 2, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.model(img)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        validity = self.adv_layer(x)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Descriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc): Linear(in_features=100, out_features=8192, bias=True)\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train G Loss: 0.7535, Train D Loss: 0.6503\n",
      "Epoch [2/200], Train G Loss: 0.9038, Train D Loss: 0.5882\n",
      "Epoch [3/200], Train G Loss: 0.9960, Train D Loss: 0.5532\n",
      "Epoch [4/200], Train G Loss: 1.0860, Train D Loss: 0.5264\n",
      "Epoch [5/200], Train G Loss: 1.1196, Train D Loss: 0.5185\n",
      "Epoch [6/200], Train G Loss: 1.1534, Train D Loss: 0.5080\n",
      "Epoch [7/200], Train G Loss: 1.2054, Train D Loss: 0.4951\n",
      "Epoch [8/200], Train G Loss: 1.2627, Train D Loss: 0.4848\n",
      "Epoch [9/200], Train G Loss: 1.2725, Train D Loss: 0.4715\n",
      "Epoch [10/200], Train G Loss: 1.3162, Train D Loss: 0.4731\n",
      "Epoch [11/200], Train G Loss: 1.3545, Train D Loss: 0.4612\n",
      "Epoch [12/200], Train G Loss: 1.3612, Train D Loss: 0.4628\n",
      "Epoch [13/200], Train G Loss: 1.3908, Train D Loss: 0.4605\n",
      "Epoch [14/200], Train G Loss: 1.4238, Train D Loss: 0.4378\n",
      "Epoch [15/200], Train G Loss: 1.4731, Train D Loss: 0.4335\n",
      "Epoch [16/200], Train G Loss: 1.4985, Train D Loss: 0.4264\n",
      "Epoch [17/200], Train G Loss: 1.5483, Train D Loss: 0.4145\n",
      "Epoch [18/200], Train G Loss: 1.5792, Train D Loss: 0.4163\n",
      "Epoch [19/200], Train G Loss: 1.6191, Train D Loss: 0.3983\n",
      "Epoch [20/200], Train G Loss: 1.6253, Train D Loss: 0.3858\n",
      "Epoch [21/200], Train G Loss: 1.6620, Train D Loss: 0.3993\n",
      "Epoch [22/200], Train G Loss: 1.6918, Train D Loss: 0.3738\n",
      "Epoch [23/200], Train G Loss: 1.7366, Train D Loss: 0.3766\n",
      "Epoch [24/200], Train G Loss: 1.7349, Train D Loss: 0.3761\n",
      "Epoch [25/200], Train G Loss: 1.8453, Train D Loss: 0.3555\n",
      "Epoch [26/200], Train G Loss: 1.8130, Train D Loss: 0.3612\n",
      "Epoch [27/200], Train G Loss: 1.8961, Train D Loss: 0.3381\n",
      "Epoch [28/200], Train G Loss: 1.8856, Train D Loss: 0.3411\n",
      "Epoch [29/200], Train G Loss: 1.9686, Train D Loss: 0.3372\n",
      "Epoch [30/200], Train G Loss: 2.0218, Train D Loss: 0.3131\n",
      "Epoch [31/200], Train G Loss: 2.0652, Train D Loss: 0.3221\n",
      "Epoch [32/200], Train G Loss: 2.0665, Train D Loss: 0.3072\n",
      "Epoch [33/200], Train G Loss: 2.1677, Train D Loss: 0.3040\n",
      "Epoch [34/200], Train G Loss: 2.2792, Train D Loss: 0.2798\n",
      "Epoch [35/200], Train G Loss: 2.2023, Train D Loss: 0.2931\n",
      "Epoch [36/200], Train G Loss: 2.3230, Train D Loss: 0.2747\n",
      "Epoch [37/200], Train G Loss: 2.4591, Train D Loss: 0.2598\n",
      "Epoch [38/200], Train G Loss: 2.4366, Train D Loss: 0.2624\n",
      "Epoch [39/200], Train G Loss: 2.5639, Train D Loss: 0.2483\n",
      "Epoch [40/200], Train G Loss: 2.5450, Train D Loss: 0.2434\n",
      "Epoch [41/200], Train G Loss: 2.7180, Train D Loss: 0.2358\n",
      "Epoch [42/200], Train G Loss: 2.6451, Train D Loss: 0.2269\n",
      "Epoch [43/200], Train G Loss: 2.7891, Train D Loss: 0.2225\n",
      "Epoch [44/200], Train G Loss: 2.8249, Train D Loss: 0.2136\n",
      "Epoch [45/200], Train G Loss: 2.8715, Train D Loss: 0.2068\n",
      "Epoch [46/200], Train G Loss: 3.0332, Train D Loss: 0.2159\n",
      "Epoch [47/200], Train G Loss: 2.9577, Train D Loss: 0.2012\n",
      "Epoch [48/200], Train G Loss: 3.1604, Train D Loss: 0.2001\n",
      "Epoch [49/200], Train G Loss: 3.2597, Train D Loss: 0.1881\n",
      "Epoch [50/200], Train G Loss: 3.2122, Train D Loss: 0.1847\n",
      "Epoch [51/200], Train G Loss: 3.3101, Train D Loss: 0.1823\n",
      "Epoch [52/200], Train G Loss: 3.2349, Train D Loss: 0.1799\n",
      "Epoch [53/200], Train G Loss: 3.3206, Train D Loss: 0.1736\n",
      "Epoch [54/200], Train G Loss: 3.3861, Train D Loss: 0.1710\n",
      "Epoch [55/200], Train G Loss: 3.4813, Train D Loss: 0.1657\n",
      "Epoch [56/200], Train G Loss: 3.4525, Train D Loss: 0.1781\n",
      "Epoch [57/200], Train G Loss: 3.6614, Train D Loss: 0.1628\n",
      "Epoch [58/200], Train G Loss: 3.6327, Train D Loss: 0.1751\n",
      "Epoch [59/200], Train G Loss: 3.6330, Train D Loss: 0.1593\n",
      "Epoch [60/200], Train G Loss: 3.6822, Train D Loss: 0.1635\n",
      "Epoch [61/200], Train G Loss: 3.6863, Train D Loss: 0.1692\n",
      "Epoch [62/200], Train G Loss: 3.8137, Train D Loss: 0.1456\n",
      "Epoch [63/200], Train G Loss: 3.7904, Train D Loss: 0.1559\n",
      "Epoch [64/200], Train G Loss: 3.8284, Train D Loss: 0.1391\n",
      "Epoch [65/200], Train G Loss: 3.8323, Train D Loss: 0.1521\n",
      "Epoch [66/200], Train G Loss: 3.8841, Train D Loss: 0.1395\n",
      "Epoch [67/200], Train G Loss: 3.9814, Train D Loss: 0.1381\n",
      "Epoch [68/200], Train G Loss: 4.0434, Train D Loss: 0.1420\n",
      "Epoch [69/200], Train G Loss: 4.0312, Train D Loss: 0.1383\n",
      "Epoch [70/200], Train G Loss: 3.9695, Train D Loss: 0.1500\n",
      "Epoch [71/200], Train G Loss: 4.0809, Train D Loss: 0.1451\n",
      "Epoch [72/200], Train G Loss: 3.9838, Train D Loss: 0.1502\n",
      "Epoch [73/200], Train G Loss: 4.1302, Train D Loss: 0.1330\n",
      "Epoch [74/200], Train G Loss: 4.1709, Train D Loss: 0.1414\n",
      "Epoch [75/200], Train G Loss: 4.0804, Train D Loss: 0.1386\n",
      "Epoch [76/200], Train G Loss: 4.0917, Train D Loss: 0.1313\n",
      "Epoch [77/200], Train G Loss: 4.2805, Train D Loss: 0.1123\n",
      "Epoch [78/200], Train G Loss: 4.2976, Train D Loss: 0.1252\n",
      "Epoch [79/200], Train G Loss: 4.2143, Train D Loss: 0.1395\n",
      "Epoch [80/200], Train G Loss: 4.2307, Train D Loss: 0.1260\n",
      "Epoch [81/200], Train G Loss: 4.4104, Train D Loss: 0.1342\n",
      "Epoch [82/200], Train G Loss: 4.2269, Train D Loss: 0.1265\n",
      "Epoch [83/200], Train G Loss: 4.3164, Train D Loss: 0.1319\n",
      "Epoch [84/200], Train G Loss: 4.3447, Train D Loss: 0.1318\n",
      "Epoch [85/200], Train G Loss: 4.4011, Train D Loss: 0.1216\n",
      "Epoch [86/200], Train G Loss: 4.3820, Train D Loss: 0.1337\n",
      "Epoch [87/200], Train G Loss: 4.3741, Train D Loss: 0.1150\n",
      "Epoch [88/200], Train G Loss: 4.5623, Train D Loss: 0.1339\n",
      "Epoch [89/200], Train G Loss: 4.3397, Train D Loss: 0.1349\n",
      "Epoch [90/200], Train G Loss: 4.4982, Train D Loss: 0.1296\n",
      "Epoch [91/200], Train G Loss: 4.5732, Train D Loss: 0.1119\n",
      "Epoch [92/200], Train G Loss: 4.5269, Train D Loss: 0.1181\n",
      "Epoch [93/200], Train G Loss: 4.5632, Train D Loss: 0.1121\n",
      "Epoch [94/200], Train G Loss: 4.6062, Train D Loss: 0.1266\n",
      "Epoch [95/200], Train G Loss: 4.6289, Train D Loss: 0.1225\n",
      "Epoch [96/200], Train G Loss: 4.7060, Train D Loss: 0.1111\n",
      "Epoch [97/200], Train G Loss: 4.7896, Train D Loss: 0.1137\n",
      "Epoch [98/200], Train G Loss: 4.7577, Train D Loss: 0.1218\n",
      "Epoch [99/200], Train G Loss: 4.8041, Train D Loss: 0.1193\n",
      "Epoch [100/200], Train G Loss: 4.8255, Train D Loss: 0.1141\n",
      "Epoch [101/200], Train G Loss: 4.6854, Train D Loss: 0.1058\n",
      "Epoch [102/200], Train G Loss: 4.8103, Train D Loss: 0.1142\n",
      "Epoch [103/200], Train G Loss: 4.8318, Train D Loss: 0.1189\n",
      "Epoch [104/200], Train G Loss: 4.8202, Train D Loss: 0.1194\n",
      "Epoch [105/200], Train G Loss: 4.8310, Train D Loss: 0.1057\n",
      "Epoch [106/200], Train G Loss: 5.0401, Train D Loss: 0.1029\n",
      "Epoch [107/200], Train G Loss: 4.8866, Train D Loss: 0.0966\n",
      "Epoch [108/200], Train G Loss: 5.0736, Train D Loss: 0.1138\n",
      "Epoch [109/200], Train G Loss: 5.0689, Train D Loss: 0.1046\n",
      "Epoch [110/200], Train G Loss: 5.1561, Train D Loss: 0.0894\n",
      "Epoch [111/200], Train G Loss: 5.1456, Train D Loss: 0.0956\n",
      "Epoch [112/200], Train G Loss: 5.1396, Train D Loss: 0.0937\n",
      "Epoch [113/200], Train G Loss: 5.1947, Train D Loss: 0.0910\n",
      "Epoch [114/200], Train G Loss: 5.2183, Train D Loss: 0.0937\n",
      "Epoch [115/200], Train G Loss: 5.3860, Train D Loss: 0.0785\n",
      "Epoch [116/200], Train G Loss: 5.4326, Train D Loss: 0.0832\n",
      "Epoch [117/200], Train G Loss: 5.3376, Train D Loss: 0.0958\n",
      "Epoch [118/200], Train G Loss: 5.4265, Train D Loss: 0.1031\n",
      "Epoch [119/200], Train G Loss: 5.2581, Train D Loss: 0.0771\n",
      "Epoch [120/200], Train G Loss: 5.3974, Train D Loss: 0.0970\n",
      "Epoch [121/200], Train G Loss: 5.5068, Train D Loss: 0.0760\n",
      "Epoch [122/200], Train G Loss: 5.5605, Train D Loss: 0.0807\n",
      "Epoch [123/200], Train G Loss: 5.8310, Train D Loss: 0.0816\n",
      "Epoch [124/200], Train G Loss: 5.4813, Train D Loss: 0.0911\n",
      "Epoch [125/200], Train G Loss: 5.5850, Train D Loss: 0.0824\n",
      "Epoch [126/200], Train G Loss: 5.6269, Train D Loss: 0.0874\n",
      "Epoch [127/200], Train G Loss: 5.6550, Train D Loss: 0.0724\n",
      "Epoch [128/200], Train G Loss: 5.7142, Train D Loss: 0.0788\n",
      "Epoch [129/200], Train G Loss: 5.7270, Train D Loss: 0.0782\n",
      "Epoch [130/200], Train G Loss: 5.7726, Train D Loss: 0.0758\n",
      "Epoch [131/200], Train G Loss: 5.7608, Train D Loss: 0.0722\n",
      "Epoch [132/200], Train G Loss: 5.8177, Train D Loss: 0.0784\n",
      "Epoch [133/200], Train G Loss: 6.0727, Train D Loss: 0.0637\n",
      "Epoch [134/200], Train G Loss: 5.9555, Train D Loss: 0.0717\n",
      "Epoch [135/200], Train G Loss: 5.8363, Train D Loss: 0.0726\n",
      "Epoch [136/200], Train G Loss: 6.1654, Train D Loss: 0.0581\n",
      "Epoch [137/200], Train G Loss: 5.9059, Train D Loss: 0.0777\n",
      "Epoch [138/200], Train G Loss: 6.0956, Train D Loss: 0.0650\n",
      "Epoch [139/200], Train G Loss: 6.0531, Train D Loss: 0.0712\n",
      "Epoch [140/200], Train G Loss: 6.1065, Train D Loss: 0.0630\n",
      "Epoch [141/200], Train G Loss: 6.2159, Train D Loss: 0.0729\n",
      "Epoch [142/200], Train G Loss: 6.1811, Train D Loss: 0.0723\n",
      "Epoch [143/200], Train G Loss: 6.0545, Train D Loss: 0.0886\n",
      "Epoch [144/200], Train G Loss: 6.0915, Train D Loss: 0.0615\n",
      "Epoch [145/200], Train G Loss: 6.0485, Train D Loss: 0.0697\n",
      "Epoch [146/200], Train G Loss: 6.1811, Train D Loss: 0.0652\n",
      "Epoch [147/200], Train G Loss: 6.2704, Train D Loss: 0.0665\n",
      "Epoch [148/200], Train G Loss: 6.2281, Train D Loss: 0.0714\n",
      "Epoch [149/200], Train G Loss: 6.2621, Train D Loss: 0.0646\n",
      "Epoch [150/200], Train G Loss: 6.4494, Train D Loss: 0.0510\n",
      "Epoch [151/200], Train G Loss: 6.5841, Train D Loss: 0.0636\n",
      "Epoch [152/200], Train G Loss: 6.4704, Train D Loss: 0.0623\n",
      "Epoch [153/200], Train G Loss: 6.4051, Train D Loss: 0.0733\n",
      "Epoch [154/200], Train G Loss: 6.7117, Train D Loss: 0.0581\n",
      "Epoch [155/200], Train G Loss: 6.4050, Train D Loss: 0.0621\n",
      "Epoch [156/200], Train G Loss: 6.3494, Train D Loss: 0.0586\n",
      "Epoch [157/200], Train G Loss: 6.6173, Train D Loss: 0.0694\n",
      "Epoch [158/200], Train G Loss: 6.3519, Train D Loss: 0.0753\n",
      "Epoch [159/200], Train G Loss: 6.9041, Train D Loss: 0.0564\n",
      "Epoch [160/200], Train G Loss: 6.5933, Train D Loss: 0.0622\n",
      "Epoch [161/200], Train G Loss: 6.4649, Train D Loss: 0.0534\n",
      "Epoch [162/200], Train G Loss: 6.5617, Train D Loss: 0.0708\n",
      "Epoch [163/200], Train G Loss: 6.3191, Train D Loss: 0.0611\n",
      "Epoch [164/200], Train G Loss: 6.6430, Train D Loss: 0.0584\n",
      "Epoch [165/200], Train G Loss: 6.6548, Train D Loss: 0.0462\n",
      "Epoch [166/200], Train G Loss: 6.7947, Train D Loss: 0.0591\n",
      "Epoch [167/200], Train G Loss: 6.9949, Train D Loss: 0.0557\n",
      "Epoch [168/200], Train G Loss: 6.7924, Train D Loss: 0.0530\n",
      "Epoch [169/200], Train G Loss: 6.9979, Train D Loss: 0.0490\n",
      "Epoch [170/200], Train G Loss: 6.6289, Train D Loss: 0.0545\n",
      "Epoch [171/200], Train G Loss: 6.9035, Train D Loss: 0.0557\n",
      "Epoch [172/200], Train G Loss: 6.8860, Train D Loss: 0.0557\n",
      "Epoch [173/200], Train G Loss: 6.6881, Train D Loss: 0.0604\n",
      "Epoch [174/200], Train G Loss: 6.7365, Train D Loss: 0.0556\n",
      "Epoch [175/200], Train G Loss: 6.8303, Train D Loss: 0.0577\n",
      "Epoch [176/200], Train G Loss: 6.7058, Train D Loss: 0.0604\n",
      "Epoch [177/200], Train G Loss: 7.1603, Train D Loss: 0.0455\n",
      "Epoch [178/200], Train G Loss: 6.8600, Train D Loss: 0.0553\n",
      "Epoch [179/200], Train G Loss: 6.9887, Train D Loss: 0.0589\n",
      "Epoch [180/200], Train G Loss: 6.9347, Train D Loss: 0.0358\n",
      "Epoch [181/200], Train G Loss: 6.9957, Train D Loss: 0.0414\n",
      "Epoch [182/200], Train G Loss: 7.1062, Train D Loss: 0.0454\n",
      "Epoch [183/200], Train G Loss: 6.7326, Train D Loss: 0.0643\n",
      "Epoch [184/200], Train G Loss: 7.1698, Train D Loss: 0.0621\n",
      "Epoch [185/200], Train G Loss: 6.8672, Train D Loss: 0.0486\n",
      "Epoch [186/200], Train G Loss: 7.0948, Train D Loss: 0.0486\n",
      "Epoch [187/200], Train G Loss: 7.0982, Train D Loss: 0.0566\n",
      "Epoch [188/200], Train G Loss: 6.9909, Train D Loss: 0.0503\n",
      "Epoch [189/200], Train G Loss: 7.1325, Train D Loss: 0.0461\n",
      "Epoch [190/200], Train G Loss: 6.9743, Train D Loss: 0.0622\n",
      "Epoch [191/200], Train G Loss: 7.1857, Train D Loss: 0.0357\n",
      "Epoch [192/200], Train G Loss: 7.0945, Train D Loss: 0.0515\n",
      "Epoch [193/200], Train G Loss: 7.2097, Train D Loss: 0.0577\n",
      "Epoch [194/200], Train G Loss: 7.2804, Train D Loss: 0.0636\n",
      "Epoch [195/200], Train G Loss: 7.3953, Train D Loss: 0.0338\n",
      "Epoch [196/200], Train G Loss: 7.1961, Train D Loss: 0.0338\n",
      "Epoch [197/200], Train G Loss: 7.2993, Train D Loss: 0.0547\n",
      "Epoch [198/200], Train G Loss: 7.2335, Train D Loss: 0.0512\n",
      "Epoch [199/200], Train G Loss: 7.2277, Train D Loss: 0.0609\n",
      "Epoch [200/200], Train G Loss: 7.3612, Train D Loss: 0.0504\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "hist = {\n",
    "        \"train_G_loss\": [],\n",
    "        \"train_D_loss\": [],\n",
    "    }\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_G_loss = 0.0\n",
    "    running_D_loss = 0.0\n",
    "\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "        valid = torch.ones(imgs.shape[0], 1).to(device)\n",
    "        fake = torch.zeros(imgs.shape[0], 1).to(device)\n",
    "\n",
    "        # --- Train Generator --- \n",
    "        optimizer_G.zero_grad()\n",
    "        # Noise input for Generator\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))).to(device)\n",
    "\n",
    "        gen_imgs = generator(z)\n",
    "        G_loss = criterion(discriminator(gen_imgs), valid)\n",
    "        running_G_loss += G_loss.item()\n",
    "\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # --- Train Descriminator --- \n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        D_loss = (real_loss + fake_loss) / 2\n",
    "        running_D_loss += D_loss.item()\n",
    "\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    epoch_G_loss = running_G_loss / len(dataloader)\n",
    "    epoch_D_loss = running_D_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train G Loss: {epoch_G_loss:.4f}, Train D Loss: {epoch_D_loss:.4f}\")\n",
    "\n",
    "    hist[\"train_G_loss\"].append(epoch_G_loss)\n",
    "    hist[\"train_D_loss\"].append(epoch_D_loss)\n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"images/epoch_{epoch}.png\", nrow=5, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
