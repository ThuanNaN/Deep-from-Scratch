{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "img_size = 32\n",
    "    \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((img_size, img_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5],\n",
    "                                     std=[0.5])\n",
    "])\n",
    "\n",
    "images = torchvision.datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataloader = torch.utils.data.DataLoader(images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "img_shape = (channels, img_size, img_size)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Descriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Descriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (12): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train G Loss: 1.1855, Train D Loss: 0.4254\n",
      "Epoch [2/200], Train G Loss: 1.6886, Train D Loss: 0.3632\n",
      "Epoch [3/200], Train G Loss: 1.7938, Train D Loss: 0.3640\n",
      "Epoch [4/200], Train G Loss: 1.9682, Train D Loss: 0.3350\n",
      "Epoch [5/200], Train G Loss: 2.1366, Train D Loss: 0.3066\n",
      "Epoch [6/200], Train G Loss: 2.1757, Train D Loss: 0.3021\n",
      "Epoch [7/200], Train G Loss: 2.2030, Train D Loss: 0.3033\n",
      "Epoch [8/200], Train G Loss: 2.3021, Train D Loss: 0.2936\n",
      "Epoch [9/200], Train G Loss: 2.4095, Train D Loss: 0.2781\n",
      "Epoch [10/200], Train G Loss: 2.4214, Train D Loss: 0.2767\n",
      "Epoch [11/200], Train G Loss: 2.4791, Train D Loss: 0.2683\n",
      "Epoch [12/200], Train G Loss: 2.4148, Train D Loss: 0.2725\n",
      "Epoch [13/200], Train G Loss: 2.4165, Train D Loss: 0.2828\n",
      "Epoch [14/200], Train G Loss: 2.3369, Train D Loss: 0.2827\n",
      "Epoch [15/200], Train G Loss: 2.2245, Train D Loss: 0.3088\n",
      "Epoch [16/200], Train G Loss: 2.1508, Train D Loss: 0.3156\n",
      "Epoch [17/200], Train G Loss: 2.0428, Train D Loss: 0.3360\n",
      "Epoch [18/200], Train G Loss: 2.0432, Train D Loss: 0.3334\n",
      "Epoch [19/200], Train G Loss: 1.9323, Train D Loss: 0.3509\n",
      "Epoch [20/200], Train G Loss: 1.9268, Train D Loss: 0.3621\n",
      "Epoch [21/200], Train G Loss: 1.7711, Train D Loss: 0.3856\n",
      "Epoch [22/200], Train G Loss: 1.7409, Train D Loss: 0.3901\n",
      "Epoch [23/200], Train G Loss: 1.7511, Train D Loss: 0.3899\n",
      "Epoch [24/200], Train G Loss: 1.7245, Train D Loss: 0.3917\n",
      "Epoch [25/200], Train G Loss: 1.6800, Train D Loss: 0.4015\n",
      "Epoch [26/200], Train G Loss: 1.7329, Train D Loss: 0.3918\n",
      "Epoch [27/200], Train G Loss: 1.7191, Train D Loss: 0.3943\n",
      "Epoch [28/200], Train G Loss: 1.7679, Train D Loss: 0.3785\n",
      "Epoch [29/200], Train G Loss: 1.7376, Train D Loss: 0.3843\n",
      "Epoch [30/200], Train G Loss: 1.7964, Train D Loss: 0.3751\n",
      "Epoch [31/200], Train G Loss: 1.8134, Train D Loss: 0.3704\n",
      "Epoch [32/200], Train G Loss: 1.8528, Train D Loss: 0.3555\n",
      "Epoch [33/200], Train G Loss: 1.8804, Train D Loss: 0.3542\n",
      "Epoch [34/200], Train G Loss: 1.9158, Train D Loss: 0.3559\n",
      "Epoch [35/200], Train G Loss: 1.9020, Train D Loss: 0.3520\n",
      "Epoch [36/200], Train G Loss: 1.9452, Train D Loss: 0.3452\n",
      "Epoch [37/200], Train G Loss: 1.9950, Train D Loss: 0.3350\n",
      "Epoch [38/200], Train G Loss: 1.9949, Train D Loss: 0.3391\n",
      "Epoch [39/200], Train G Loss: 2.0295, Train D Loss: 0.3284\n",
      "Epoch [40/200], Train G Loss: 2.0741, Train D Loss: 0.3257\n",
      "Epoch [41/200], Train G Loss: 2.0772, Train D Loss: 0.3271\n",
      "Epoch [42/200], Train G Loss: 2.1275, Train D Loss: 0.3138\n",
      "Epoch [43/200], Train G Loss: 2.1450, Train D Loss: 0.3125\n",
      "Epoch [44/200], Train G Loss: 2.1644, Train D Loss: 0.3104\n",
      "Epoch [45/200], Train G Loss: 2.1572, Train D Loss: 0.3099\n",
      "Epoch [46/200], Train G Loss: 2.2021, Train D Loss: 0.3033\n",
      "Epoch [47/200], Train G Loss: 2.2260, Train D Loss: 0.2999\n",
      "Epoch [48/200], Train G Loss: 2.2016, Train D Loss: 0.3027\n",
      "Epoch [49/200], Train G Loss: 2.2290, Train D Loss: 0.2982\n",
      "Epoch [50/200], Train G Loss: 2.2425, Train D Loss: 0.2919\n",
      "Epoch [51/200], Train G Loss: 2.3274, Train D Loss: 0.2845\n",
      "Epoch [52/200], Train G Loss: 2.2990, Train D Loss: 0.2878\n",
      "Epoch [53/200], Train G Loss: 2.3054, Train D Loss: 0.2858\n",
      "Epoch [54/200], Train G Loss: 2.3223, Train D Loss: 0.2844\n",
      "Epoch [55/200], Train G Loss: 2.3622, Train D Loss: 0.2762\n",
      "Epoch [56/200], Train G Loss: 2.3395, Train D Loss: 0.2811\n",
      "Epoch [57/200], Train G Loss: 2.3510, Train D Loss: 0.2761\n",
      "Epoch [58/200], Train G Loss: 2.3862, Train D Loss: 0.2650\n",
      "Epoch [59/200], Train G Loss: 2.4107, Train D Loss: 0.2722\n",
      "Epoch [60/200], Train G Loss: 2.4225, Train D Loss: 0.2665\n",
      "Epoch [61/200], Train G Loss: 2.4334, Train D Loss: 0.2645\n",
      "Epoch [62/200], Train G Loss: 2.4646, Train D Loss: 0.2627\n",
      "Epoch [63/200], Train G Loss: 2.4811, Train D Loss: 0.2596\n",
      "Epoch [64/200], Train G Loss: 2.4594, Train D Loss: 0.2600\n",
      "Epoch [65/200], Train G Loss: 2.4791, Train D Loss: 0.2576\n",
      "Epoch [66/200], Train G Loss: 2.4614, Train D Loss: 0.2564\n",
      "Epoch [67/200], Train G Loss: 2.4709, Train D Loss: 0.2541\n",
      "Epoch [68/200], Train G Loss: 2.4880, Train D Loss: 0.2535\n",
      "Epoch [69/200], Train G Loss: 2.5298, Train D Loss: 0.2487\n",
      "Epoch [70/200], Train G Loss: 2.5167, Train D Loss: 0.2499\n",
      "Epoch [71/200], Train G Loss: 2.5340, Train D Loss: 0.2439\n",
      "Epoch [72/200], Train G Loss: 2.5189, Train D Loss: 0.2481\n",
      "Epoch [73/200], Train G Loss: 2.5551, Train D Loss: 0.2451\n",
      "Epoch [74/200], Train G Loss: 2.5305, Train D Loss: 0.2460\n",
      "Epoch [75/200], Train G Loss: 2.5259, Train D Loss: 0.2392\n",
      "Epoch [76/200], Train G Loss: 2.5499, Train D Loss: 0.2460\n",
      "Epoch [77/200], Train G Loss: 2.5626, Train D Loss: 0.2430\n",
      "Epoch [78/200], Train G Loss: 2.5712, Train D Loss: 0.2384\n",
      "Epoch [79/200], Train G Loss: 2.5860, Train D Loss: 0.2293\n",
      "Epoch [80/200], Train G Loss: 2.5549, Train D Loss: 0.2352\n",
      "Epoch [81/200], Train G Loss: 2.5610, Train D Loss: 0.2343\n",
      "Epoch [82/200], Train G Loss: 2.5422, Train D Loss: 0.2356\n",
      "Epoch [83/200], Train G Loss: 2.5730, Train D Loss: 0.2274\n",
      "Epoch [84/200], Train G Loss: 2.5588, Train D Loss: 0.2308\n",
      "Epoch [85/200], Train G Loss: 2.5705, Train D Loss: 0.2314\n",
      "Epoch [86/200], Train G Loss: 2.5777, Train D Loss: 0.2297\n",
      "Epoch [87/200], Train G Loss: 2.5614, Train D Loss: 0.2296\n",
      "Epoch [88/200], Train G Loss: 2.5557, Train D Loss: 0.2311\n",
      "Epoch [89/200], Train G Loss: 2.5489, Train D Loss: 0.2310\n",
      "Epoch [90/200], Train G Loss: 2.5466, Train D Loss: 0.2282\n",
      "Epoch [91/200], Train G Loss: 2.5529, Train D Loss: 0.2271\n",
      "Epoch [92/200], Train G Loss: 2.5260, Train D Loss: 0.2290\n",
      "Epoch [93/200], Train G Loss: 2.5413, Train D Loss: 0.2301\n",
      "Epoch [94/200], Train G Loss: 2.5305, Train D Loss: 0.2260\n",
      "Epoch [95/200], Train G Loss: 2.5590, Train D Loss: 0.2263\n",
      "Epoch [96/200], Train G Loss: 2.5351, Train D Loss: 0.2279\n",
      "Epoch [97/200], Train G Loss: 2.5546, Train D Loss: 0.2256\n",
      "Epoch [98/200], Train G Loss: 2.5254, Train D Loss: 0.2249\n",
      "Epoch [99/200], Train G Loss: 2.5431, Train D Loss: 0.2255\n",
      "Epoch [100/200], Train G Loss: 2.5327, Train D Loss: 0.2269\n",
      "Epoch [101/200], Train G Loss: 2.5309, Train D Loss: 0.2248\n",
      "Epoch [102/200], Train G Loss: 2.5196, Train D Loss: 0.2237\n",
      "Epoch [103/200], Train G Loss: 2.4977, Train D Loss: 0.2257\n",
      "Epoch [104/200], Train G Loss: 2.5029, Train D Loss: 0.2237\n",
      "Epoch [105/200], Train G Loss: 2.5265, Train D Loss: 0.2240\n",
      "Epoch [106/200], Train G Loss: 2.5152, Train D Loss: 0.2233\n",
      "Epoch [107/200], Train G Loss: 2.5014, Train D Loss: 0.2240\n",
      "Epoch [108/200], Train G Loss: 2.4963, Train D Loss: 0.2243\n",
      "Epoch [109/200], Train G Loss: 2.5012, Train D Loss: 0.2251\n",
      "Epoch [110/200], Train G Loss: 2.4988, Train D Loss: 0.2249\n",
      "Epoch [111/200], Train G Loss: 2.5160, Train D Loss: 0.2218\n",
      "Epoch [112/200], Train G Loss: 2.4870, Train D Loss: 0.2239\n",
      "Epoch [113/200], Train G Loss: 2.5196, Train D Loss: 0.2226\n",
      "Epoch [114/200], Train G Loss: 2.5158, Train D Loss: 0.2220\n",
      "Epoch [115/200], Train G Loss: 2.4951, Train D Loss: 0.2219\n",
      "Epoch [116/200], Train G Loss: 2.4990, Train D Loss: 0.2235\n",
      "Epoch [117/200], Train G Loss: 2.5008, Train D Loss: 0.2216\n",
      "Epoch [118/200], Train G Loss: 2.4991, Train D Loss: 0.2215\n",
      "Epoch [119/200], Train G Loss: 2.4857, Train D Loss: 0.2233\n",
      "Epoch [120/200], Train G Loss: 2.4770, Train D Loss: 0.2224\n",
      "Epoch [121/200], Train G Loss: 2.4824, Train D Loss: 0.2192\n",
      "Epoch [122/200], Train G Loss: 2.4878, Train D Loss: 0.2211\n",
      "Epoch [123/200], Train G Loss: 2.4929, Train D Loss: 0.2208\n",
      "Epoch [124/200], Train G Loss: 2.4832, Train D Loss: 0.2229\n",
      "Epoch [125/200], Train G Loss: 2.4826, Train D Loss: 0.2200\n",
      "Epoch [126/200], Train G Loss: 2.4924, Train D Loss: 0.2207\n",
      "Epoch [127/200], Train G Loss: 2.4864, Train D Loss: 0.2200\n",
      "Epoch [128/200], Train G Loss: 2.4880, Train D Loss: 0.2170\n",
      "Epoch [129/200], Train G Loss: 2.4817, Train D Loss: 0.2199\n",
      "Epoch [130/200], Train G Loss: 2.4685, Train D Loss: 0.2204\n",
      "Epoch [131/200], Train G Loss: 2.4746, Train D Loss: 0.2191\n",
      "Epoch [132/200], Train G Loss: 2.4821, Train D Loss: 0.2179\n",
      "Epoch [133/200], Train G Loss: 2.4855, Train D Loss: 0.2178\n",
      "Epoch [134/200], Train G Loss: 2.4887, Train D Loss: 0.2172\n",
      "Epoch [135/200], Train G Loss: 2.4732, Train D Loss: 0.2175\n",
      "Epoch [136/200], Train G Loss: 2.4685, Train D Loss: 0.2192\n",
      "Epoch [137/200], Train G Loss: 2.4678, Train D Loss: 0.2195\n",
      "Epoch [138/200], Train G Loss: 2.4733, Train D Loss: 0.2172\n",
      "Epoch [139/200], Train G Loss: 2.4705, Train D Loss: 0.2207\n",
      "Epoch [140/200], Train G Loss: 2.4652, Train D Loss: 0.2180\n",
      "Epoch [141/200], Train G Loss: 2.4740, Train D Loss: 0.2191\n",
      "Epoch [142/200], Train G Loss: 2.4544, Train D Loss: 0.2195\n",
      "Epoch [143/200], Train G Loss: 2.4621, Train D Loss: 0.2174\n",
      "Epoch [144/200], Train G Loss: 2.4668, Train D Loss: 0.2157\n",
      "Epoch [145/200], Train G Loss: 2.4630, Train D Loss: 0.2165\n",
      "Epoch [146/200], Train G Loss: 2.4612, Train D Loss: 0.2205\n",
      "Epoch [147/200], Train G Loss: 2.4580, Train D Loss: 0.2164\n",
      "Epoch [148/200], Train G Loss: 2.4611, Train D Loss: 0.2154\n",
      "Epoch [149/200], Train G Loss: 2.4586, Train D Loss: 0.2173\n",
      "Epoch [150/200], Train G Loss: 2.4541, Train D Loss: 0.2203\n",
      "Epoch [151/200], Train G Loss: 2.4551, Train D Loss: 0.2160\n",
      "Epoch [152/200], Train G Loss: 2.4483, Train D Loss: 0.2154\n",
      "Epoch [153/200], Train G Loss: 2.4656, Train D Loss: 0.2159\n",
      "Epoch [154/200], Train G Loss: 2.4623, Train D Loss: 0.2136\n",
      "Epoch [155/200], Train G Loss: 2.4609, Train D Loss: 0.2176\n",
      "Epoch [156/200], Train G Loss: 2.4692, Train D Loss: 0.2135\n",
      "Epoch [157/200], Train G Loss: 2.4655, Train D Loss: 0.2154\n",
      "Epoch [158/200], Train G Loss: 2.4440, Train D Loss: 0.2207\n",
      "Epoch [159/200], Train G Loss: 2.4453, Train D Loss: 0.2148\n",
      "Epoch [160/200], Train G Loss: 2.4555, Train D Loss: 0.2168\n",
      "Epoch [161/200], Train G Loss: 2.4483, Train D Loss: 0.2155\n",
      "Epoch [162/200], Train G Loss: 2.4551, Train D Loss: 0.2156\n",
      "Epoch [163/200], Train G Loss: 2.4520, Train D Loss: 0.2162\n",
      "Epoch [164/200], Train G Loss: 2.4489, Train D Loss: 0.2144\n",
      "Epoch [165/200], Train G Loss: 2.4570, Train D Loss: 0.2160\n",
      "Epoch [166/200], Train G Loss: 2.4440, Train D Loss: 0.2149\n",
      "Epoch [167/200], Train G Loss: 2.4706, Train D Loss: 0.2130\n",
      "Epoch [168/200], Train G Loss: 2.4527, Train D Loss: 0.2132\n",
      "Epoch [169/200], Train G Loss: 2.4628, Train D Loss: 0.2128\n",
      "Epoch [170/200], Train G Loss: 2.4673, Train D Loss: 0.2124\n",
      "Epoch [171/200], Train G Loss: 2.4574, Train D Loss: 0.2146\n",
      "Epoch [172/200], Train G Loss: 2.4504, Train D Loss: 0.2131\n",
      "Epoch [173/200], Train G Loss: 2.4689, Train D Loss: 0.2121\n",
      "Epoch [174/200], Train G Loss: 2.4504, Train D Loss: 0.2123\n",
      "Epoch [175/200], Train G Loss: 2.4531, Train D Loss: 0.2130\n",
      "Epoch [176/200], Train G Loss: 2.4704, Train D Loss: 0.2106\n",
      "Epoch [177/200], Train G Loss: 2.4541, Train D Loss: 0.2136\n",
      "Epoch [178/200], Train G Loss: 2.4502, Train D Loss: 0.2117\n",
      "Epoch [179/200], Train G Loss: 2.4562, Train D Loss: 0.2139\n",
      "Epoch [180/200], Train G Loss: 2.4660, Train D Loss: 0.2113\n",
      "Epoch [181/200], Train G Loss: 2.4658, Train D Loss: 0.2107\n",
      "Epoch [182/200], Train G Loss: 2.4712, Train D Loss: 0.2096\n",
      "Epoch [183/200], Train G Loss: 2.4653, Train D Loss: 0.2083\n",
      "Epoch [184/200], Train G Loss: 2.4711, Train D Loss: 0.2120\n",
      "Epoch [185/200], Train G Loss: 2.4760, Train D Loss: 0.2084\n",
      "Epoch [186/200], Train G Loss: 2.4699, Train D Loss: 0.2094\n",
      "Epoch [187/200], Train G Loss: 2.4786, Train D Loss: 0.2099\n",
      "Epoch [188/200], Train G Loss: 2.4735, Train D Loss: 0.2073\n",
      "Epoch [189/200], Train G Loss: 2.4703, Train D Loss: 0.2100\n",
      "Epoch [190/200], Train G Loss: 2.4754, Train D Loss: 0.2096\n",
      "Epoch [191/200], Train G Loss: 2.4655, Train D Loss: 0.2084\n",
      "Epoch [192/200], Train G Loss: 2.4766, Train D Loss: 0.2075\n",
      "Epoch [193/200], Train G Loss: 2.4821, Train D Loss: 0.2087\n",
      "Epoch [194/200], Train G Loss: 2.4746, Train D Loss: 0.2094\n",
      "Epoch [195/200], Train G Loss: 2.4764, Train D Loss: 0.2071\n",
      "Epoch [196/200], Train G Loss: 2.4761, Train D Loss: 0.2089\n",
      "Epoch [197/200], Train G Loss: 2.4699, Train D Loss: 0.2099\n",
      "Epoch [198/200], Train G Loss: 2.4810, Train D Loss: 0.2078\n",
      "Epoch [199/200], Train G Loss: 2.4821, Train D Loss: 0.2066\n",
      "Epoch [200/200], Train G Loss: 2.4677, Train D Loss: 0.2074\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "hist = {\n",
    "        \"train_G_loss\": [],\n",
    "        \"train_D_loss\": [],\n",
    "    }\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_G_loss = 0.0\n",
    "    running_D_loss = 0.0\n",
    "\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "        valid = torch.ones(imgs.shape[0], 1).to(device)\n",
    "        fake = torch.zeros(imgs.shape[0], 1).to(device)\n",
    "\n",
    "        # --- Train Generator --- \n",
    "        optimizer_G.zero_grad()\n",
    "        # Noise input for Generator\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))).to(device)\n",
    "\n",
    "        gen_imgs = generator(z)\n",
    "        G_loss = criterion(discriminator(gen_imgs), valid)\n",
    "        running_G_loss += G_loss.item()\n",
    "\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # --- Train Descriminator --- \n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        D_loss = (real_loss + fake_loss) / 2\n",
    "        running_D_loss += D_loss.item()\n",
    "\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    \n",
    "    epoch_G_loss = running_G_loss / len(dataloader)\n",
    "    epoch_D_loss = running_D_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train G Loss: {epoch_G_loss:.4f}, Train D Loss: {epoch_D_loss:.4f}\")\n",
    "\n",
    "    hist[\"train_G_loss\"].append(epoch_G_loss)\n",
    "    hist[\"train_D_loss\"].append(epoch_D_loss)\n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"images/epoch_{epoch}.png\", nrow=5, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
