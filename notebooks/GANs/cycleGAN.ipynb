{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_name = \"monet2photo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash download_cyclegan_dataset.sh monet2photo\n",
    "\n",
    "# !mkdir -p \"./data/monet2photo/train\" \"./data/monet2photo/test\"\n",
    "\n",
    "# !mv \"./data/monet2photo/trainA\" \"./data/monet2photo/train/A\"\n",
    "# !mv \"./data/monet2photo/trainB\" \"./data/monet2photo/train/B\"\n",
    "# !mv \"./data/monet2photo/testA\" \"./data/monet2photo/test/A\"\n",
    "# !mv \"./data/monet2photo/testB\" \"./data/monet2photo/test/B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None, unaligned=False, split=\"train\"):\n",
    "        self.transform = torchvision.transforms.Compose(transforms)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, f\"{split}/A\") + \"/*.*\"))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, f\"{split}/B\") + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # grayscale to rgb\n",
    "        if image_A.mode != \"RGB\":\n",
    "            image_A = self.to_rgb(image_A)\n",
    "        if image_B.mode != \"RGB\":\n",
    "            image_B = self.to_rgb(image_B)\n",
    "\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "\n",
    "    def _to_rgb(self, image):\n",
    "        rgb_image = Image.new(\"RGB\", image.size)\n",
    "        rgb_image.paste(image)\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "transforms = [\n",
    "    torchvision.transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    torchvision.transforms.RandomCrop((img_height, img_width)),\n",
    "    # torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "train_set = ImageDataset(f\"data/{dataset_name}\", transforms=transforms, unaligned=True, split=\"train\")\n",
    "test_set = ImageDataset(f\"data/{dataset_name}\", transforms=transforms, unaligned=True, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "        out_features = 64\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_features, out_features, normalize=True):\n",
    "            layers = [nn.Conv2d(in_features, out_features, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_features))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, img_height, img_width)\n",
    "n_residual_blocks = 3\n",
    "\n",
    "G_AB = Generator(input_shape, n_residual_blocks).to(device)\n",
    "G_BA = Generator(input_shape, n_residual_blocks).to(device)\n",
    "D_A = Discriminator(input_shape).to(device)\n",
    "D_B = Discriminator(input_shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    imgs = next(iter(val_loader))\n",
    "    real_A = imgs[\"B\"][:5, ...].to(device)\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = imgs[\"A\"][:5, ...].to(device)\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, f\"images/{batches_done}.png\", normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "LR = 0.0002\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), \n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=LR, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:06<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train G Loss: 3.9555, Train D Loss: 0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 786/786 [08:44<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Train G Loss: 3.3419, Train D Loss: 0.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:59<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Train G Loss: 3.1046, Train D Loss: 0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Train G Loss: 2.9587, Train D Loss: 0.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:03<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Train G Loss: 2.8967, Train D Loss: 0.1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Train G Loss: 2.8164, Train D Loss: 0.1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:43<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Train G Loss: 2.6104, Train D Loss: 0.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:04<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Train G Loss: 2.6902, Train D Loss: 0.1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Train G Loss: 2.6428, Train D Loss: 0.1615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Train G Loss: 2.4746, Train D Loss: 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Train G Loss: 2.4874, Train D Loss: 0.1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Train G Loss: 2.4891, Train D Loss: 0.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Train G Loss: 2.5007, Train D Loss: 0.1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Train G Loss: 2.4246, Train D Loss: 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Train G Loss: 2.4810, Train D Loss: 0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Train G Loss: 2.5664, Train D Loss: 0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:01<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Train G Loss: 2.7438, Train D Loss: 0.2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:12<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Train G Loss: 2.2985, Train D Loss: 0.1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Train G Loss: 2.3552, Train D Loss: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Train G Loss: 2.4146, Train D Loss: 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Train G Loss: 2.7186, Train D Loss: 0.1772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Train G Loss: 2.4311, Train D Loss: 0.1609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Train G Loss: 2.2624, Train D Loss: 0.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Train G Loss: 2.2002, Train D Loss: 0.1638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Train G Loss: 2.2276, Train D Loss: 0.1674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Train G Loss: 2.2860, Train D Loss: 0.1396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Train G Loss: 2.2700, Train D Loss: 0.1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Train G Loss: 2.2473, Train D Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Train G Loss: 2.2445, Train D Loss: 0.1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Train G Loss: 2.2673, Train D Loss: 0.1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Train G Loss: 2.6849, Train D Loss: 0.2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Train G Loss: 2.2374, Train D Loss: 0.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Train G Loss: 1.9957, Train D Loss: 0.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Train G Loss: 2.0515, Train D Loss: 0.1661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Train G Loss: 2.1081, Train D Loss: 0.1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Train G Loss: 2.1753, Train D Loss: 0.1199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:48<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Train G Loss: 2.2081, Train D Loss: 0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [09:23<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Train G Loss: 2.2252, Train D Loss: 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:43<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Train G Loss: 2.2443, Train D Loss: 0.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Train G Loss: 2.2628, Train D Loss: 0.0903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Train G Loss: 2.2633, Train D Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Train G Loss: 2.6482, Train D Loss: 0.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Train G Loss: 2.2444, Train D Loss: 0.2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Train G Loss: 2.1789, Train D Loss: 0.1477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Train G Loss: 2.0243, Train D Loss: 0.1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Train G Loss: 2.0692, Train D Loss: 0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Train G Loss: 2.1126, Train D Loss: 0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Train G Loss: 2.1241, Train D Loss: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Train G Loss: 2.1386, Train D Loss: 0.1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [08:38<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Train G Loss: 2.3898, Train D Loss: 0.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "save_interval = 10\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "hist = {\n",
    "        \"train_G_loss\": [],\n",
    "        \"train_D_loss\": [],\n",
    "}\n",
    "for epoch in range(EPOCHS):\n",
    "    running_G_loss = 0.0\n",
    "    running_D_loss = 0.0\n",
    "\n",
    "    for batch in tqdm.tqdm(train_loader, total=len(train_loader)):\n",
    "\n",
    "        real_imgs_A = batch[\"B\"].to(device)\n",
    "        real_imgs_B = batch[\"A\"].to(device)\n",
    "\n",
    "        valid = Tensor(np.ones((real_imgs_A.size(0), *D_A.output_shape))).to(device)\n",
    "        fake = Tensor(np.zeros((real_imgs_A.size(0), *D_A.output_shape))).to(device)\n",
    "\n",
    "        # --- Train Generator ---\n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_imgs_A), real_imgs_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_imgs_B), real_imgs_B)\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "        \n",
    "        # GAN loss\n",
    "        fake_imgs_B = G_AB(real_imgs_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_imgs_B), valid)\n",
    "        fake_imgs_A = G_BA(real_imgs_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_imgs_A), valid)\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_imgs_A = G_BA(fake_imgs_B)\n",
    "        loss_cycle_A = criterion_cycle(recovered_imgs_A, real_imgs_A)\n",
    "        recovered_imgs_B = G_AB(fake_imgs_A)\n",
    "        loss_cycle_B = criterion_cycle(recovered_imgs_B, real_imgs_B)\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + loss_cycle*lambda_cyc + loss_identity*lambda_id\n",
    "        running_G_loss += loss_G.item()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "        # --- Train Discriminator A ---\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        loss_real = criterion_GAN(D_A(real_imgs_A), valid)\n",
    "        \n",
    "        fake_A_ = fake_A_buffer.push_and_pop(fake_imgs_A)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A_), fake)\n",
    "        \n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # --- Train Discriminator B ---\n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "        loss_real = criterion_GAN(D_B(real_imgs_B), valid)\n",
    "\n",
    "        fake_B_ = fake_B_buffer.push_and_pop(fake_imgs_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_), fake)\n",
    "\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # Total loss\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "        running_D_loss += loss_D.item()\n",
    "\n",
    "    epoch_G_loss = running_G_loss / len(train_loader)\n",
    "    epoch_D_loss = running_D_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train G Loss: {epoch_G_loss:.4f}, Train D Loss: {epoch_D_loss:.4f}\")\n",
    "\n",
    "    hist[\"train_G_loss\"].append(epoch_G_loss)\n",
    "    hist[\"train_D_loss\"].append(epoch_D_loss)\n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        sample_images(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "torch.save(G_AB.state_dict(), \"saved_models/G_AB_%d.pth\" % ( epoch))\n",
    "torch.save(G_BA.state_dict(), \"saved_models/G_BA_%d.pth\" % ( epoch))\n",
    "torch.save(D_A.state_dict(), \"saved_models/D_A_%d.pth\" % (epoch))\n",
    "torch.save(D_B.state_dict(), \"saved_models/D_B_%d.pth\" % ( epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
