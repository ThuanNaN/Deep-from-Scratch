{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "d_model = 32 # Embedding dim\n",
    "n_head = 4\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is all you need paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "$MultiHead(Q, K, V) =  Concat(head_{1}, ..., head_{h})W^{O}$\n",
    "\n",
    "$head_{i}=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$\n",
    "\n",
    "$W^{Q}_{i} \\in R^{(d_{model}, d_{k})}$\n",
    "\n",
    "$W^{K}_{i} \\in R^{(d_{model}, d_{k})}$\n",
    "\n",
    "$W^{V}_{i} \\in R^{(d_{model}, d_{v})}$\n",
    "\n",
    "$d_{k} = d_{v} = d_{model} / h $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make same with paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_SelfAttention_(nn.Module):\n",
    "    def __init__(self, d_model, n_head, bias=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_model // n_head\n",
    "        self.d_v = d_model // n_head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, T, C = q.size() # batch size, sequence length, embedding dimension\n",
    "\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.d_k).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, self.d_k).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, self.d_v).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att_map = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))\n",
    "\n",
    "        if mask is not None:\n",
    "            pass\n",
    "        \n",
    "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        attention = F.softmax(att_map, dim=-1) @ v \n",
    "        \n",
    "        output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output = self.w_o(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_layer = MultiHead_SelfAttention_(d_model=d_model, n_head=n_head)\n",
    "\n",
    "for w in test_layer.parameters():\n",
    "    print(w.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make same with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, bias=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_model // n_head\n",
    "        self.d_v = d_model // n_head\n",
    "        \n",
    "        ### >>> Different with above class >>>\n",
    "        self.c_attn = nn.Linear(d_model, 3*d_model, bias=bias) \n",
    "        ### >>>\n",
    "\n",
    "        self.c_proj = nn.Linear(d_model, d_model, bias=bias) # same with above class\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, T, C = q.size() # batch size, sequence length, embedding dimension\n",
    "\n",
    "        ### >>> Different with above class >>>\n",
    "        q, k, v  = self.c_attn(q).split(self.d_model, dim=2)\n",
    "        ### >>>\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.d_k).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, self.d_k).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, self.d_v).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att_map = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))\n",
    "\n",
    "        if mask is not None:\n",
    "            pass\n",
    "        \n",
    "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        attention = F.softmax(att_map, dim=-1) @ v \n",
    "        \n",
    "        output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output = self.c_proj(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_att_layer = MultiHead_SelfAttention(d_model=d_model, n_head=n_head, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 32])\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "for w in hand_att_layer.parameters():\n",
    "    print(w.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pytorch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_att_layer = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_head, bias=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 32])\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "for w in torch_att_layer.parameters():\n",
    "    print(w.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying weight from torch_att_layer to hand_att_layer\n",
    "with torch.no_grad():\n",
    "    hand_att_layer.c_attn.weight = deepcopy(torch_att_layer.in_proj_weight)\n",
    "    hand_att_layer.c_proj.weight = deepcopy(torch_att_layer.out_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = np.random.randint(low=0, high=90000, size=(batch_size, seq_len, d_model))\n",
    "batch_data = torch.Tensor(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = hand_att_layer(batch_data, batch_data, batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2, _ = torch_att_layer(batch_data, batch_data, batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allow different with different from -0.1 to +0.1\n",
    "torch.all(torch.lt(torch.abs(torch.add(output_1, - output_2)), 1e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tensorflow (Keras) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.keras.Input(shape=[128, 32])\n",
    "layer = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=2, use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = layer(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "weights = layer.get_weights()\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 2)\n",
      "(32, 1, 2)\n",
      "(32, 1, 2)\n",
      "(1, 2, 32)\n"
     ]
    }
   ],
   "source": [
    "print(weights[0].shape)\n",
    "print(weights[1].shape)\n",
    "print(weights[2].shape)\n",
    "print(weights[3].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
